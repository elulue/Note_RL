{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设老虎机标准差为 2 倍均值\n",
    "# 假设所获得的奖励符合高斯分布\n",
    "BANDITS = [0.10, -0.20, 0.0, -5.0, 5.0, -20.0, 10.0, 3.0, -2.0, -1.0]\n",
    "def bandit_reward(bandit):\n",
    "    # 所有 bandits 的期望\n",
    "    mu = BANDITS[bandit]\n",
    "    sigma = np.abs(mu)*2.0\n",
    "    return np.random.normal(mu, sigma, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每次拉老虎机，随机从奖励结果中取一个\n",
    "def pull_bandit(bandit):\n",
    "    banditreward = bandit_reward(bandit)\n",
    "    choose = np.random.randint(0,len(BANDITS)-1)\n",
    "    return banditreward[choose]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual reward: 0.1; Estimated reward: 0.10269837746044579\n",
      "Actual reward: -0.2; Estimated reward: -0.1834657223039294\n",
      "Actual reward: 0.0; Estimated reward: 0.0\n",
      "Actual reward: -5.0; Estimated reward: -4.775941324626682\n",
      "Actual reward: 5.0; Estimated reward: 5.161489818103677\n",
      "Actual reward: -20.0; Estimated reward: -17.692357382026564\n",
      "Actual reward: 10.0; Estimated reward: 10.102715068772545\n",
      "Actual reward: 3.0; Estimated reward: 3.229079401489951\n",
      "Actual reward: -2.0; Estimated reward: -2.020107165639584\n",
      "Actual reward: -1.0; Estimated reward: -0.948306270504664\n"
     ]
    }
   ],
   "source": [
    "# 验证一下, 可以发现基本能够达到期望值\n",
    "for i, rew in enumerate(BANDITS):\n",
    "    result = []\n",
    "    # 重复 1000 次\n",
    "    for j in range(1000):\n",
    "        result.append(pull_bandit(i))\n",
    "    print(\"Actual reward: %s; Estimated reward: %s\" % (rew, np.average(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获得的总收益：14180，你选择的老虎机期望收益：10.0...\n",
      "\n",
      "你估计的 Q_table: [0.1, -0.3, 0.0, 0.4, 0.2, -44.8, 14.7, 6.5, -1.0, -1.7], 你选择额的最大 Q: 14.7, 7 号老虎机。\n",
      "实际中的 Q_table: [0.1, -0.2, 0.0, -5.0, 5.0, -20.0, 10.0, 3.0, -2.0, -1.0]\n",
      "\n",
      "You have chosen the best bandit!\n"
     ]
    }
   ],
   "source": [
    "# 贪心算法\n",
    "\n",
    "total_reward = 0\n",
    "try_reward = []\n",
    "trial = 3\n",
    "ite = 0\n",
    "total_iter = 1000\n",
    "while ite < total_iter:\n",
    "    # 各 三次 实验求平均，然后以贪心算法选择最大的 Q\n",
    "    if ite < trial * len(BANDITS):\n",
    "        for i in range(len(BANDITS)):\n",
    "            ite += 1\n",
    "            reward = round(pull_bandit(i),1)\n",
    "            try_reward.append(reward)\n",
    "            total_reward += reward\n",
    "    else:\n",
    "        Q_table = np.average(np.array(try_reward).reshape(trial, len(BANDITS)), axis=0)\n",
    "        # 保留两位小数\n",
    "        Q_table = list(np.around(Q_table, decimals=1))\n",
    "        Q = max(Q_table)\n",
    "        index = Q_table.index(Q)\n",
    "        ite += 1\n",
    "        total_reward += Q\n",
    "\n",
    "assert sum(try_reward) + Q*(total_iter - trial * len(BANDITS)) - total_reward < 1.0\n",
    "print(\"获得的总收益：%d，你选择的老虎机期望收益：%s...\\n\" % (total_reward, BANDITS[index]))\n",
    "print(\"你估计的 Q_table: %s, 你选择额的最大 Q: %s, %s 号老虎机。\" % (Q_table, Q, index+1))\n",
    "print(\"实际中的 Q_table: %s\\n\" % BANDITS)\n",
    "if index == bandits.index(max(BANDITS)):\n",
    "    print(\"You have chosen the best bandit!\")\n",
    "else:\n",
    "    print(\"You haven't chosen the best bandit....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贪心算法小结：  \n",
    "\n",
    "注意这个结果是和 bandits 的期望以及标准差相关的。  \n",
    "\n",
    "- 期望一定时：如果标准差很小，那么选择基本会限定在最高的几个老虎机上，如果标准差非常大，则有可能选择到其他的老虎机。\n",
    "- 标准差一定时：期望相差越大，越容易选到最高的几个老虎机上，期望相差越小，则可能选到其他老虎机。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# epson-贪心算法\n",
    "\n",
    "def epson_greedy(epson):\n",
    "    total_reward = 0\n",
    "    epson_num = 0\n",
    "    try_reward = []\n",
    "    trial = 3\n",
    "    ite = 0\n",
    "    total_iter = 1000\n",
    "    while ite < total_iter:\n",
    "        \n",
    "        # 先三次实验确定 Q_table，再探索\n",
    "        if ite < trial * len(BANDITS):\n",
    "            for i in range(len(BANDITS)):\n",
    "                ite += 1\n",
    "                reward = round(pull_bandit(i),1)\n",
    "                try_reward.append(reward)\n",
    "                total_reward += reward\n",
    "        \n",
    "        # 上来就开始探索，第一次随机选一个\n",
    "#         if ite < 1:\n",
    "#             ite += 1\n",
    "#             choose = np.random.randint(0,len(BANDITS)-1)\n",
    "#             Q_table[choose] = round(pull_bandit(choose), 1)\n",
    "        \n",
    "        else:\n",
    "            # 选择先 trial 次实验确定 Q_table 时，需要下面两句 计算 Q_table\n",
    "            # 选择上来就开始探索时，不需要\n",
    "            Q_table = np.average(np.array(try_reward).reshape(trial, len(BANDITS)), axis=0)\n",
    "            Q_table = list(np.around(Q_table, decimals=1))\n",
    "            \n",
    "            # epson 概率选择下一个要拉的老虎机，1-epson 选目前奖励最高的 \n",
    "            # 如果是非静态的 bandit，可以把 Q_table 中每台 bandit 的 reward 存成 list，然后\n",
    "            # 用 Q_{n+1} = \\sum_{i=1}^n \\alpha (1-\\alpha)^{n-i} R_i + (1-\\alpha)^{n} Q_1 计算即可\n",
    "            # alpha 为超参数，R_i 为存储的所有 Reward，Q1 为第一次的 Reward\n",
    "            if random.random() < epson:\n",
    "                epson_num += 1\n",
    "                Q_table_new = Q_table\n",
    "                choose = np.random.randint(0,len(BANDITS)-1)\n",
    "                new_rew = pull_bandit(choose)\n",
    "                # 更新的值保留两位小数，方便看结果\n",
    "                Q_table_new[choose] = round(new_rew, 2)\n",
    "                Q = max(Q_table_new)\n",
    "                index = Q_table_new.index(Q)\n",
    "            # 保留两位小数\n",
    "            else:\n",
    "                Q = max(Q_table)\n",
    "                index = Q_table.index(Q)\n",
    "            ite += 1\n",
    "            total_reward += Q\n",
    "\n",
    "    print(\"共探索了 %s 次.\" % epson_num)\n",
    "    print(\"获得的总收益：%d，你选择的老虎机期望收益：%s...\\n\" % (total_reward, BANDITS[index]))\n",
    "    # 如果你选的 Q（两位小数）没有在 Q_table，说明最后一次的 Q_table 是 Q_table_new，最大的 Q 是探索出来的\n",
    "    # 实际中不需要 新建一个 Qtable，这里为了看起来更加直观\n",
    "    print(\"你估计的 Q_table: %s, 你选择额的最大 Q: %s, %s 号老虎机。\" % (Q_table, Q, index+1))\n",
    "    print(\"实际中的 Q_table: %s\\n\" % BANDITS)\n",
    "    if index == BANDITS.index(max(BANDITS)):\n",
    "        print(\"You have chosen the best bandit!\")\n",
    "    else:\n",
    "        print(\"You haven't chosen the best bandit....\")\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共探索了 853 次.\n",
      "获得的总收益：15446，你选择的老虎机期望收益：10.0...\n",
      "\n",
      "你估计的 Q_table: [0.1, -0.4, 0.0, 1.6, 3.9, -8.8, 21.31, 7.0, 0.2, -0.8], 你选择额的最大 Q: 21.31, 7 号老虎机。\n",
      "实际中的 Q_table: [0.1, -0.2, 0.0, -5.0, 5.0, -20.0, 10.0, 3.0, -2.0, -1.0]\n",
      "\n",
      "You have chosen the best bandit!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15446.799999999781"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epson = 0.1，10% 的概率在探索新的老虎机\n",
    "epson_greedy(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimistic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper-confidence bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
